{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune_alexnet_solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mancinimassimiliano/DeepLearningLab/blob/master/Lab3/solution/finetune_alexnet_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "r6Qlk71Hctpl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mount your Google drive folder on Colab\n",
        "\n",
        "We are going to put data in google drive and then copy from google drive to colab local drive."
      ]
    },
    {
      "metadata": {
        "id": "dr1yEuKSPrpq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xoqaDOLSdEKD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download dataset\n",
        "\n",
        "After mounting the drive we will download OfficeHome dataset. \n",
        "\n",
        "Add [this](https://drive.google.com/file/d/10xMJi5uD8kVh9xkksq1pXngpUl-ckRBa/view?usp=sharing) .tar file to your Unitn google drive.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "m695n9_jStkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copy the .tar file from gdrive to local drive. This will make data loading faster.\n",
        "\n",
        "First create a directory with `!mkdir dataset` in your current path"
      ]
    },
    {
      "metadata": {
        "id": "RQ0TvpzVTTKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0i67CZuS33kF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp \"gdrive/My Drive/datasets/OfficeHomeDataset.tar\" dataset/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QJUhE3lDQ7WL",
        "colab_type": "code",
        "outputId": "6a4caf0b-fc7a-43cc-efaa-a277f688cd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OfficeHomeDataset.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ujV7O9KzTfoH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unzip the .tar file\n",
        "\n",
        "```\n",
        "!tar -xf OfficeHomeDataset.tar\n",
        "```\n",
        "    Wait till its unzipped."
      ]
    },
    {
      "metadata": {
        "id": "Nw8evR3OTuDI",
        "colab_type": "code",
        "outputId": "e433d498-4f43-40e7-b6f0-fdbf8dcc6b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "ls dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mOfficeHomeDataset_10072016\u001b[0m/  OfficeHomeDataset.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KysfwRi-ema5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unzip here\n",
        "!tar -xf OfficeHomeDataset.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xxJU3U2qeqG3",
        "colab_type": "code",
        "outputId": "b096c6e5-d644-4342-8a7f-2331c78d8535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Library needed for visualization purposes\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "# Instantiate visualizer\n",
        "tb = TensorBoardColab(graph_path='./log')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://2663caa2.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eAw9WPMdgZzU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Alexnet \n",
        "\n",
        "![Alexnet architecture](https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0106.png =850x250)\n",
        "\n",
        "This is the network architecture of [Alexnet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf). In this tutorial we are going to finetune a pre-trained Alexnet for classifying Office-Home images. Alexnet has been pre-trained on ILSVRC-2012 dataset, a dataset that has more than **1 million** images with 1000 classes.\n",
        "\n",
        "***A little bit about the Office-Home dataset.***\n",
        "\n",
        "Office Home has images from 4 different domains with each domain having 65 distinct categories. In this lab session we are going to use the `Real World` domain.\n",
        "\n",
        "![Office-Home](http://hemanthdv.github.io/profile/images/DataCollage.jpg)\n",
        "\n",
        "***Steps for Fine tuning Alexnet***\n",
        "\n",
        "1.   Discard the final layer (or the output layer) of Alexnet which contains 1000 output neurons. \n",
        "2.   Randomly initialize the final layer with the number of output categories present in the dataset using `torch.nn.Linear`. In this case its 65 because OfficeHome has 65 classes. Keep all the other layers intact.\n",
        "3. Train the network with a low learning rate for the pre-initialized layers and and a higher learning rate for the newly initialized layer. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EyDL2C5EOmrt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### How to load datasets from folders containing images\n",
        "\n",
        "We have already seen that PyTorch's `torchvision` package provides some very basic dataloaders like `torchvision.datasets.MNIST`, `torchvision.datasets.SVHN`, etc. But it might happen (and is frequently the case) that we need to load our own datasets from folders.\n",
        "\n",
        "PyTorch is kind enough to provide `torchvision.datasets.ImageFolder` for loading datasets from folders with just one line of code. But we need to ensure that the images in the folders are arranged in a certain way. A sample format is shown below:\n",
        "\n",
        "OfficeHomeDataset\n",
        "\n",
        "        |\n",
        "        |--- Alarm_Clock\n",
        "        |                \n",
        "        |      |--- 00046.jpg\n",
        "        |      |--- 00050.jpg\n",
        "        |          \n",
        "        |--- Couch\n",
        "               | --- 00007.jpg\n",
        "               | --- 00023.jpg\n",
        " \n",
        " In other words, the parent folder (*OfficeHomeDataset*) contains the sub-folders (e.g *Alarm_Clock*, *Couch*). These sub-folders are the *classes*. Further, each sub-folder contains a bunch of images (eg. 00046.jpg, 00050.jpg for the class *Alarm_Clock*). Internally, PyTorch will assign a class label to each sub-folder and will also load the corresponding images.\n",
        " \n",
        " More details goes [here](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder).\n",
        " \n",
        "N.B. Your own folder structure might look different, so provide path to your parent folder accordingly."
      ]
    },
    {
      "metadata": {
        "id": "VEy32rMeVgx7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the function that fetches a data loader that is then used during iterative training.\n",
        "\n",
        "We are going to see some more PyTorch data transformations during loading the data."
      ]
    },
    {
      "metadata": {
        "id": "h8gyumMki8A6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments:\n",
        "  batch_size: mini batch size used during training\n",
        "  img_root: path to the dataset parent folder. \n",
        "            The folder just above the sub-folders or class folders\n",
        "'''\n",
        "\n",
        "def get_data(batch_size, img_root):\n",
        "  \n",
        "  # Prepare data transformations for the train loader\n",
        "  transform = list()\n",
        "  transform.append(T.Resize((256, 256)))                      # Resize each PIL image to 256 x 256\n",
        "  transform.append(T.RandomCrop((224, 224)))                  # Randonly crop a 224 x 224 patch\n",
        "  transform.append(T.ToTensor())                              # converts Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                               std=[0.229, 0.224, 0.225]))    # Normalize with ImageNet mean\n",
        "  transform = T.Compose(transform)                            # Composes the above transformations into one.\n",
        "    \n",
        "  # Load data\n",
        "  officehome_dataset = torchvision.datasets.ImageFolder(root=img_root, transform=transform)\n",
        "  \n",
        "  # Create train and test splits\n",
        "  # We will create a 80:20 % train:test split\n",
        "  num_samples = len(officehome_dataset)\n",
        "  training_samples = int(num_samples * 0.8 + 1)\n",
        "  test_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, test_data = torch.utils.data.random_split(officehome_dataset, \n",
        "                                                           [training_samples, test_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False)\n",
        "  \n",
        "  return train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LQfpbouSan0O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Alexnet model\n",
        "\n",
        "PyTorch provides a bunch of pre-trained models whose parameters have been learned on ImageNet dataset. All these models can be found [here](https://pytorch.org/docs/stable/torchvision/models.html).\n",
        "\n",
        "We will load a pre-trained Alexnet using PyTorch's `torchvision` package. Then we will re-initialize the output layer suited for our classification task.\n",
        "\n",
        "Before that lets have a look at the [code](https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py) of Alexnet."
      ]
    },
    {
      "metadata": {
        "id": "XqUYywc6eH2d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments\n",
        "  num_classes: number of classes in the dataset.\n",
        "               This is equal to the number of output neurons.\n",
        "'''\n",
        "\n",
        "def initialize_alexnet(num_classes):\n",
        "  # load the pre-trained Alexnet\n",
        "  alexnet = torchvision.models.alexnet(pretrained=True)\n",
        "  \n",
        "  # get the number of neurons in the penultimate layer\n",
        "  in_features = alexnet.classifier[6].in_features\n",
        "  \n",
        "  # re-initalize the output layer\n",
        "  alexnet.classifier[6] = torch.nn.Linear(in_features=in_features, \n",
        "                                          out_features=num_classes)\n",
        "  \n",
        "  return alexnet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7RSi4GnMnv3U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize our custom Alexnet model"
      ]
    },
    {
      "metadata": {
        "id": "QI4VtFdVnzBg",
        "colab_type": "code",
        "outputId": "e96154f9-0570-4828-f7b8-731578fe3445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "print(initialize_alexnet(65))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Dropout(p=0.5)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): Linear(in_features=4096, out_features=65, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dvNjbRebk9X-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define cost function"
      ]
    },
    {
      "metadata": {
        "id": "i5SGHT3YlAHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPoVyKnmlEXu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the optimizer\n",
        "\n",
        "The optimizer will be different from the previous experiments.\n",
        "\n",
        "Previously the *learning rate* of all the layers of the network was the same.\n",
        "\n",
        "Now, we will have different layers learning at a different rate. The pre-trained layers need to be updated at a lesser rate than the newly initialized layer. Details are available [here](https://pytorch.org/docs/stable/optim.html)."
      ]
    },
    {
      "metadata": {
        "id": "LoqZLU0DlJ_N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  \n",
        "  # we will create two groups of weights, one for the newly initialized layer\n",
        "  # and the other for rest of the layers of the network\n",
        "  \n",
        "  final_layer_weights = []\n",
        "  rest_of_the_net_weights = []\n",
        "  \n",
        "  # we will iterate through the layers of the network\n",
        "  for name, param in model.named_parameters():\n",
        "    if name.startswith('classifier.6'):\n",
        "      final_layer_weights.append(param)\n",
        "    else:\n",
        "      rest_of_the_net_weights.append(param)\n",
        "  \n",
        "  # so now we have divided the network weights into two groups.\n",
        "  # We will train the final_layer_weights with learning_rate = lr\n",
        "  # and rest_of_the_net_weights with learning_rate = lr / 10\n",
        "  \n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': rest_of_the_net_weights},\n",
        "      {'params': final_layer_weights, 'lr': lr}\n",
        "  ], lr= lr / 10, weight_decay = wd, momentum = momentum)\n",
        "  \n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qW2kNnSkkxO3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and test functions"
      ]
    },
    {
      "metadata": {
        "id": "skcjnASok3w7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  \n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # Reset the optimizer\n",
        "      \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_quVeuqqM8U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Wrapping everything up\n",
        "\n",
        "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "metadata": {
        "id": "FK8A9alWqYZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "  num_classes: Number of classes in your dataset\n",
        "  visualization_name: Name of the visualization folder\n",
        "  img_root: The root folder of images\n",
        "'''\n",
        "\n",
        "def main(batch_size=128, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50, \n",
        "         num_classes=65, \n",
        "         visualization_name='alexnet_sgd', \n",
        "         img_root=None):\n",
        "  \n",
        "  train_loader, test_loader = get_data(batch_size=batch_size, \n",
        "                                       img_root=img_root)\n",
        "  \n",
        "  net = initialize_alexnet(num_classes=num_classes).to(device)\n",
        "  \n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  \n",
        "  # Add values to plots\n",
        "  tb.save_value('Loss/train_loss', visualization_name, 0, train_loss)\n",
        "  tb.save_value('Loss/test_loss', visualization_name, 0, test_loss)\n",
        "  tb.save_value('Accuracy/train_accuracy', visualization_name, 0, train_accuracy)\n",
        "  tb.save_value('Accuracy/test_accuracy', visualization_name, 0, test_accuracy)\n",
        "    \n",
        "  # Update plots \n",
        "  tb.flush_line(visualization_name)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    # Add values to plots\n",
        "    tb.save_value('Loss/train_loss', visualization_name, e + 1, train_loss)\n",
        "    tb.save_value('Loss/test_loss', visualization_name, e + 1, test_loss)\n",
        "    tb.save_value('Accuracy/train_accuracy', visualization_name, se + 1, train_accuracy)\n",
        "    tb.save_value('Accuracy/test_accuracy', visualization_name, e + 1, test_accuracy)\n",
        "    \n",
        "    # Update plots \n",
        "    tb.flush_line(visualization_name)\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D-gT1YH4evqm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train!"
      ]
    },
    {
      "metadata": {
        "id": "-TrjNXtBq891",
        "colab_type": "code",
        "outputId": "c3732c02-4af4-4264-e222-43e3edf3e4d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2681
        }
      },
      "cell_type": "code",
      "source": [
        "main(visualization_name='alexnet_sgd_0.01_RW', \n",
        "     img_root = '/content/dataset/OfficeHomeDataset_10072016/Real World')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before training:\n",
            "\t Training loss 0.03591, Training accuracy 1.15\n",
            "\t Test loss 0.03612, Test accuracy 1.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.02808, Training accuracy 19.36\n",
            "\t Test loss 0.01872, Test accuracy 45.35\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.01381, Training accuracy 57.40\n",
            "\t Test loss 0.01213, Test accuracy 60.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.00930, Training accuracy 68.10\n",
            "\t Test loss 0.01024, Test accuracy 64.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.00737, Training accuracy 74.44\n",
            "\t Test loss 0.00965, Test accuracy 66.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.00593, Training accuracy 79.92\n",
            "\t Test loss 0.00932, Test accuracy 67.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00490, Training accuracy 82.33\n",
            "\t Test loss 0.00913, Test accuracy 68.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00419, Training accuracy 85.51\n",
            "\t Test loss 0.00914, Test accuracy 67.85\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00374, Training accuracy 86.20\n",
            "\t Test loss 0.00906, Test accuracy 68.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00326, Training accuracy 88.04\n",
            "\t Test loss 0.00900, Test accuracy 68.89\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00281, Training accuracy 90.39\n",
            "\t Test loss 0.00911, Test accuracy 70.03\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00244, Training accuracy 91.51\n",
            "\t Test loss 0.00929, Test accuracy 68.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00218, Training accuracy 92.54\n",
            "\t Test loss 0.00935, Test accuracy 68.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00189, Training accuracy 93.32\n",
            "\t Test loss 0.00952, Test accuracy 68.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00176, Training accuracy 94.32\n",
            "\t Test loss 0.00955, Test accuracy 68.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00157, Training accuracy 95.09\n",
            "\t Test loss 0.00983, Test accuracy 67.97\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00147, Training accuracy 94.87\n",
            "\t Test loss 0.01007, Test accuracy 69.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00132, Training accuracy 95.61\n",
            "\t Test loss 0.01014, Test accuracy 68.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00119, Training accuracy 96.07\n",
            "\t Test loss 0.01000, Test accuracy 69.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.00116, Training accuracy 96.24\n",
            "\t Test loss 0.01039, Test accuracy 66.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.00111, Training accuracy 96.64\n",
            "\t Test loss 0.01007, Test accuracy 68.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 21\n",
            "\t Training loss 0.00104, Training accuracy 96.87\n",
            "\t Test loss 0.01082, Test accuracy 67.74\n",
            "-----------------------------------------------------\n",
            "Epoch: 22\n",
            "\t Training loss 0.00098, Training accuracy 97.16\n",
            "\t Test loss 0.01042, Test accuracy 68.54\n",
            "-----------------------------------------------------\n",
            "Epoch: 23\n",
            "\t Training loss 0.00086, Training accuracy 97.65\n",
            "\t Test loss 0.01034, Test accuracy 68.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 24\n",
            "\t Training loss 0.00080, Training accuracy 97.82\n",
            "\t Test loss 0.01046, Test accuracy 67.97\n",
            "-----------------------------------------------------\n",
            "Epoch: 25\n",
            "\t Training loss 0.00079, Training accuracy 97.65\n",
            "\t Test loss 0.01062, Test accuracy 69.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 26\n",
            "\t Training loss 0.00072, Training accuracy 97.62\n",
            "\t Test loss 0.01114, Test accuracy 68.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 27\n",
            "\t Training loss 0.00068, Training accuracy 97.68\n",
            "\t Test loss 0.01073, Test accuracy 69.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 28\n",
            "\t Training loss 0.00067, Training accuracy 98.22\n",
            "\t Test loss 0.01103, Test accuracy 69.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 29\n",
            "\t Training loss 0.00060, Training accuracy 98.22\n",
            "\t Test loss 0.01062, Test accuracy 70.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 30\n",
            "\t Training loss 0.00060, Training accuracy 98.45\n",
            "\t Test loss 0.01105, Test accuracy 68.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 31\n",
            "\t Training loss 0.00073, Training accuracy 97.56\n",
            "\t Test loss 0.01095, Test accuracy 67.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 32\n",
            "\t Training loss 0.00054, Training accuracy 98.39\n",
            "\t Test loss 0.01125, Test accuracy 68.54\n",
            "-----------------------------------------------------\n",
            "Epoch: 33\n",
            "\t Training loss 0.00058, Training accuracy 98.39\n",
            "\t Test loss 0.01136, Test accuracy 68.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 34\n",
            "\t Training loss 0.00053, Training accuracy 98.05\n",
            "\t Test loss 0.01124, Test accuracy 68.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 35\n",
            "\t Training loss 0.00057, Training accuracy 98.34\n",
            "\t Test loss 0.01150, Test accuracy 67.85\n",
            "-----------------------------------------------------\n",
            "Epoch: 36\n",
            "\t Training loss 0.00053, Training accuracy 98.51\n",
            "\t Test loss 0.01143, Test accuracy 67.97\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UvTOa7dbiEGz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rm -rf log"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}