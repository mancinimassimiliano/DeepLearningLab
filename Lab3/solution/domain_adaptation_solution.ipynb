{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "domain_adaptation_solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mancinimassimiliano/DeepLearningLab/blob/master/Lab3/solution/domain_adaptation_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CeDheQHyMCR7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bDvRWBAiCOgq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the network with Domain Alignment Layers.\n",
        "\n",
        "Each DA layer consist of domain specific Batch Normalization Layers followed by domain agnostic scale-shift operation. Note that the domain specific BN layer will accumulate the domain specific first and second order statistics, i.e., mean and std. It can achieved by setting track_running_stats=True.\n",
        "\n",
        "Details for BN can be found [here](https://pytorch.org/docs/stable/nn.html#batchnorm2d).\n",
        "\n",
        "### Network architecture adopted from [here](http://proceedings.mlr.press/v37/ganin15-supp.pdf)\n",
        "{Conv(3, 64, 5, 2) -> DA -> ReLU } ->MaxPool(3,2) -> {Conv(64, 64, 5, 2) -> DA -> ReLU } -> MaxPool(3,2) -> {Conv(64, 128, 5, 2) -> DA -> ReLU } -> {Linear(6272, 3072) -> DA -> ReLU -> Dropout} ->  {Linear(3072, 2048) -> DA -> ReLU -> Dropout} -> {Linear(2048, 10) -> DA}"
      ]
    },
    {
      "metadata": {
        "id": "45Fmwkl8CLG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DIALNet(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(DIALNet, self).__init__()\n",
        "\t\tself.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "\t\tself.bns1 = nn.BatchNorm2d(64, affine=False)\n",
        "\t\tself.bnt1 = nn.BatchNorm2d(64, affine=False)\n",
        "\t\tself.gamma1 = nn.Parameter(torch.ones(64, 1, 1))\n",
        "\t\tself.beta1 = nn.Parameter(torch.zeros(64, 1, 1))\n",
        "\n",
        "\t\tself.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
        "\t\tself.bns2 = nn.BatchNorm2d(64, affine=False)\n",
        "\t\tself.bnt2 = nn.BatchNorm2d(64, affine=False)\n",
        "\t\tself.gamma2 = nn.Parameter(torch.ones(64, 1, 1))\n",
        "\t\tself.beta2 = nn.Parameter(torch.zeros(64, 1, 1))\n",
        "\n",
        "\t\tself.conv3 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "\t\tself.bns3 = nn.BatchNorm2d(128, affine=False)\n",
        "\t\tself.bnt3 = nn.BatchNorm2d(128, affine=False)\n",
        "\t\tself.gamma3 = nn.Parameter(torch.ones(128, 1, 1))\n",
        "\t\tself.beta3 = nn.Parameter(torch.zeros(128, 1, 1))\n",
        "\n",
        "\t\tself.fc4 = nn.Linear(6272, 3072)\n",
        "\t\tself.bns4 = nn.BatchNorm1d(3072, affine=False)\n",
        "\t\tself.bnt4 = nn.BatchNorm1d(3072, affine=False)\n",
        "\t\tself.gamma4 = nn.Parameter(torch.ones(1, 3072))\n",
        "\t\tself.beta4 = nn.Parameter(torch.zeros(1, 3072))\n",
        "\n",
        "\t\tself.fc5 = nn.Linear(3072, 2048)\n",
        "\t\tself.bns5 = nn.BatchNorm1d(2048, affine=False)\n",
        "\t\tself.bnt5 = nn.BatchNorm1d(2048, affine=False)\n",
        "\t\tself.gamma5 = nn.Parameter(torch.ones(1, 2048))\n",
        "\t\tself.beta5 = nn.Parameter(torch.zeros(1, 2048))\n",
        "\n",
        "\t\tself.fc6 = nn.Linear(2048, 10)\n",
        "\t\tself.bns6 = nn.BatchNorm1d(10, affine=False)\n",
        "\t\tself.bnt6 = nn.BatchNorm1d(10, affine=False)\n",
        "\t\tself.gamma6 = nn.Parameter(torch.ones(1, 10))\n",
        "\t\tself.beta6 = nn.Parameter(torch.zeros(1, 10))\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\tif self.training:\n",
        "\t\t\tx = self.conv1(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = F.max_pool2d(F.relu(torch.cat((self.bns1(x_source), self.bnt1(x_target)), dim=0)*self.gamma1 + self.beta1), \n",
        "                       kernel_size=3, stride=2)\n",
        "\n",
        "\t\t\tx = self.conv2(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = F.max_pool2d(F.relu(torch.cat((self.bns2(x_source), self.bnt2(x_target)), dim=0)*self.gamma2 + self.beta2), \n",
        "                       kernel_size=3, stride=2)\n",
        "\n",
        "\t\t\tx = self.conv3(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = F.relu(torch.cat((self.bns3(x_source), self.bnt3(x_target)), dim=0)*self.gamma3 + self.beta3)\n",
        "\n",
        "\t\t\tx = x.view(x.shape[0], -1)\n",
        "\t\t\tx = self.fc4(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = F.dropout(F.relu(torch.cat((self.bns4(x_source), self.bnt4(x_target)), dim=0)*self.gamma4 + self.beta4), \n",
        "                    training=self.training)\n",
        "\n",
        "\t\t\tx = self.fc5(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = F.dropout(F.relu(torch.cat((self.bns5(x_source), self.bnt5(x_target)), dim=0)*self.gamma5 + self.beta5), \n",
        "                    training=self.training)\n",
        "\n",
        "\t\t\tx = self.fc6(x)\n",
        "\t\t\tx_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "\t\t\tx = torch.cat((self.bns6(x_source), self.bnt6(x_target)), dim=0)*self.gamma6 + self.beta6\n",
        "\t\telse:\n",
        "\t\t\tx = self.conv1(x)\n",
        "\t\t\tx = F.max_pool2d(F.relu(self.bnt1(x)*self.gamma1 + self.beta1), kernel_size=3, stride=2)\n",
        "\n",
        "\t\t\tx = self.conv2(x)\n",
        "\t\t\tx = F.max_pool2d(F.relu(self.bnt2(x)*self.gamma2 + self.beta2), kernel_size=3, stride=2)\n",
        "\n",
        "\t\t\tx = self.conv3(x)\n",
        "\t\t\tx = F.relu(self.bnt3(x)*self.gamma3 + self.beta3)\n",
        "\n",
        "\t\t\tx = x.view(x.shape[0], -1)\n",
        "\t\t\tx = self.fc4(x)\n",
        "\t\t\tx = F.dropout(F.relu(self.bnt4(x)*self.gamma4 + self.beta4), training=self.training)\n",
        "\n",
        "\t\t\tx = self.fc5(x)\n",
        "\t\t\tx = F.dropout(F.relu(self.bnt5(x)*self.gamma5 + self.beta5), training=self.training)\n",
        "\n",
        "\t\t\tx = self.fc6(x)\n",
        "\t\t\tx = self.bnt6(x)*self.gamma6 + self.beta6\n",
        "\t\treturn x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vggSe0nOHE3l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define cross-entropy loss function\n",
        "\n",
        "For the labeled source data we can compute the cross entropy loss\n",
        "\n",
        "$L^{s}(\\theta) = - \\frac{1}{m} \\displaystyle \\sum^{m}_{i=1} y_{i}\\log{p_i}$\n",
        "\n",
        "where $p_i$ is the output from the network for the $i^{th}$ input and $y_i$ is the corresponding ground truth."
      ]
    },
    {
      "metadata": {
        "id": "0kAlRo5gG5cl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_ce_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7xt4NUHGJoeA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Entropy loss function\n",
        "\n",
        "For the unlabeled target samples we can no longer compute the cross entropy loss because the labels are not available. Instead we will compute the Entropy loss, which is defined as follows:\n",
        "\n",
        "$L^{t}(\\theta) = - \\frac{1}{m} \\displaystyle \\sum^{m}_{i=1} p_{i}\\log{p_i}$\n",
        "\n",
        "where $p_i$ is the output from the network for the $i^{th}$ input.\n",
        "\n",
        "For more details read [this](https://arxiv.org/pdf/1704.08082.pdf)."
      ]
    },
    {
      "metadata": {
        "id": "4ExbsbDBJvB2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_entropy_loss(x):\n",
        "  p = F.softmax(x, dim=1)\n",
        "  q = F.log_softmax(x, dim=1)\n",
        "  b = p * q\n",
        "  b = -1.0 * b.sum(-1).mean()\n",
        "  return b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zgwU6qhHH75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the optimizer"
      ]
    },
    {
      "metadata": {
        "id": "whEOWuuQHKjo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6xrNlVOfHP3f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test function"
      ]
    },
    {
      "metadata": {
        "id": "CblMCosjHMud",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nTqK-0TNHZ1f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train function"
      ]
    },
    {
      "metadata": {
        "id": "zL3suQDeHcAU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(net, source_data_loader, target_data_loader, optimizer, \n",
        "          get_ce_cost_function, entropy_loss_weight, device='cuda:0'):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_ce_loss = 0.\n",
        "  cumulative_en_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  \n",
        "  target_iter = iter(target_data_loader)\n",
        "\n",
        "  # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  net.train()\n",
        "  for batch_idx, (inputs_source, targets) in enumerate(source_data_loader):\n",
        "    \n",
        "    try:\n",
        "      inputs_target, _ = next(target_iter)\n",
        "    except:\n",
        "      target_iter = iter(target_data_loader)\n",
        "      inputs_target, _ = next(target_iter)\n",
        "    \n",
        "    inputs = torch.cat((inputs_source, inputs_target), dim=0)\n",
        "    \n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "    \n",
        "    # split the source and target outputs\n",
        "    source_output, target_output = torch.split(outputs, \n",
        "                                               split_size_or_sections=outputs.shape[0] // 2, \n",
        "                                               dim=0)\n",
        "    \n",
        "    # Apply the losses\n",
        "    ce_loss = get_ce_cost_function(source_output,targets)\n",
        "    en_loss = get_entropy_loss(target_output)\n",
        "    \n",
        "    loss = ce_loss + entropy_loss_weight * en_loss\n",
        "    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    source_samples+=inputs_source.shape[0]\n",
        "    target_samples+=inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_ce_loss += ce_loss.item()\n",
        "    cumulative_en_loss += en_loss.item()\n",
        "    _, predicted = source_output.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_ce_loss/source_samples, cumulative_en_loss/target_samples, cumulative_accuracy/source_samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l6ZjgtVXOAaC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the function that fetches a data loader that is then used during iterative training."
      ]
    },
    {
      "metadata": {
        "id": "AAc-CQmDOFdN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "  \n",
        "  # Prepare data transformations and then combine them sequentially\n",
        "  transform_mnist = list()\n",
        "  transform_mnist.append(T.ToTensor())                                              # converts Numpy to Pytorch Tensor\n",
        "  transform_mnist.append(T.Lambda(lambda x: F.pad(x, (2, 2, 2, 2), 'constant', 0))) # pad zeros to make MNIST 32 x 32\n",
        "  transform_mnist.append(T.Lambda(lambda x: x.repeat(3, 1, 1)))                     # to make MNIST RGB instead of grayscale\n",
        "  transform_mnist.append(T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))    # Normalizes the Tensors between [-1, 1]\n",
        "  transform_mnist = T.Compose(transform_mnist)                                      # Composes the above transformations into one.\n",
        "  \n",
        "  transform_svhn = list()\n",
        "  transform_svhn.append(T.ToTensor())                                              # converts Numpy to Pytorch Tensor\n",
        "  transform_svhn.append(T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))    # Normalizes the Tensors between [-1, 1]\n",
        "  transform_svhn = T.Compose(transform_svhn)                                       # Composes the above transformations into one.\n",
        "  \n",
        "  # Prepare datasets\n",
        "  \n",
        "  # Load SVHN\n",
        "  source_training_data = torchvision.datasets.SVHN('./data/svhn', split='train', transform=transform_svhn, download=True) \n",
        "  \n",
        "  # Load MNIST\n",
        "  target_training_data = torchvision.datasets.MNIST('./data/mnist', train=True, transform=transform_mnist, download=True) \n",
        "  target_test_data = torchvision.datasets.MNIST('./data/mnist', train=False, transform=transform_mnist, download=True)\n",
        "  \n",
        "  # Initialize dataloaders\n",
        "  source_train_loader = torch.utils.data.DataLoader(source_training_data, batch_size, shuffle=True, drop_last=True)\n",
        "  target_train_loader = torch.utils.data.DataLoader(target_training_data, batch_size, shuffle=True, drop_last=True)\n",
        "  \n",
        "  target_test_loader = torch.utils.data.DataLoader(target_test_data, test_batch_size, shuffle=False)\n",
        "  \n",
        "  return source_train_loader, target_train_loader, target_test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWJbK4npPlkw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Wrapping everything up\n",
        "\n",
        "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "metadata": {
        "id": "9GcXu0EiOFUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "'''\n",
        "\n",
        "def main(batch_size=32, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.01, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50,\n",
        "         entropy_loss_weight=0.1):\n",
        "  \n",
        "  source_train_loader, target_train_loader, target_test_loader = get_data(batch_size)\n",
        "  \n",
        "  net = DIALNet().to(device)\n",
        "  \n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  cost_function = get_ce_cost_function()\n",
        "  \n",
        "  for e in range(epochs):\n",
        "    train_ce_loss, train_en_loss, train_accuracy = train(net=net, source_data_loader=source_train_loader, \n",
        "                                                         target_data_loader=target_train_loader, \n",
        "                                                         optimizer=optimizer, get_ce_cost_function=cost_function,\n",
        "                                                         entropy_loss_weight=entropy_loss_weight)\n",
        "    test_loss, test_accuracy = test(net, target_test_loader, cost_function)\n",
        "    \n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Train: CE loss {:.5f}, Entropy loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_en_loss, train_accuracy))\n",
        "    print('\\t Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2I6xQzBRSM55",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train!"
      ]
    },
    {
      "metadata": {
        "id": "I9qA7yLjSO5B",
        "colab_type": "code",
        "outputId": "7c053c0c-1b08-41f1-ea07-5e013ba97396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1817
        }
      },
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "\t Train: CE loss 0.02584, Entropy loss 0.02520, Accuracy 73.38\n",
            "\t Test: CE loss 0.00447, Accuracy 75.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.01222, Entropy loss 0.00776, Accuracy 88.16\n",
            "\t Test: CE loss 0.00506, Accuracy 79.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.00974, Entropy loss 0.00506, Accuracy 90.70\n",
            "\t Test: CE loss 0.00649, Accuracy 79.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.00830, Entropy loss 0.00395, Accuracy 92.21\n",
            "\t Test: CE loss 0.00691, Accuracy 80.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00726, Entropy loss 0.00327, Accuracy 93.26\n",
            "\t Test: CE loss 0.00666, Accuracy 81.90\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00639, Entropy loss 0.00280, Accuracy 94.01\n",
            "\t Test: CE loss 0.00844, Accuracy 81.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00558, Entropy loss 0.00242, Accuracy 94.78\n",
            "\t Test: CE loss 0.00920, Accuracy 82.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00498, Entropy loss 0.00216, Accuracy 95.21\n",
            "\t Test: CE loss 0.00894, Accuracy 82.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00431, Entropy loss 0.00193, Accuracy 95.88\n",
            "\t Test: CE loss 0.00905, Accuracy 82.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00383, Entropy loss 0.00178, Accuracy 96.32\n",
            "\t Test: CE loss 0.01058, Accuracy 81.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00337, Entropy loss 0.00167, Accuracy 96.75\n",
            "\t Test: CE loss 0.01022, Accuracy 82.49\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00305, Entropy loss 0.00154, Accuracy 97.03\n",
            "\t Test: CE loss 0.01002, Accuracy 82.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00260, Entropy loss 0.00142, Accuracy 97.49\n",
            "\t Test: CE loss 0.01009, Accuracy 82.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00224, Entropy loss 0.00128, Accuracy 97.81\n",
            "\t Test: CE loss 0.01139, Accuracy 82.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00197, Entropy loss 0.00126, Accuracy 97.98\n",
            "\t Test: CE loss 0.01113, Accuracy 82.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Train: CE loss 0.00183, Entropy loss 0.00120, Accuracy 98.12\n",
            "\t Test: CE loss 0.01243, Accuracy 82.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Train: CE loss 0.00173, Entropy loss 0.00113, Accuracy 98.21\n",
            "\t Test: CE loss 0.01217, Accuracy 82.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Train: CE loss 0.00141, Entropy loss 0.00106, Accuracy 98.58\n",
            "\t Test: CE loss 0.01286, Accuracy 82.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Train: CE loss 0.00131, Entropy loss 0.00105, Accuracy 98.68\n",
            "\t Test: CE loss 0.01194, Accuracy 82.85\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Train: CE loss 0.00123, Entropy loss 0.00096, Accuracy 98.82\n",
            "\t Test: CE loss 0.01352, Accuracy 82.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 21\n",
            "\t Train: CE loss 0.00107, Entropy loss 0.00093, Accuracy 98.96\n",
            "\t Test: CE loss 0.01305, Accuracy 82.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 22\n",
            "\t Train: CE loss 0.00099, Entropy loss 0.00090, Accuracy 99.02\n",
            "\t Test: CE loss 0.01466, Accuracy 82.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 23\n",
            "\t Train: CE loss 0.00082, Entropy loss 0.00082, Accuracy 99.19\n",
            "\t Test: CE loss 0.01456, Accuracy 81.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 24\n",
            "\t Train: CE loss 0.00080, Entropy loss 0.00080, Accuracy 99.18\n",
            "\t Test: CE loss 0.01478, Accuracy 81.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 25\n",
            "\t Train: CE loss 0.00078, Entropy loss 0.00080, Accuracy 99.25\n",
            "\t Test: CE loss 0.01649, Accuracy 81.12\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}