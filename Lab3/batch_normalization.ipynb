{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "batch_normalization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mancinimassimiliano/DeepLearningLab/blob/master/Lab3/batch_normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NyDPHwTDXY5q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "### Batch Normalization from scratch\n",
        "\n",
        "In this lab session we are going to use *Batch Normalization* (BN) layers in our network. BN standardizes each feature in a mini-batch, to have 0 mean and unit variance, and then scale and shifts the standardized activations with learnable parameters. BN is known for faster convergence properties and also for improving the performance. More details about BN can be found in the original [paper](https://arxiv.org/abs/1502.03167).\n",
        "\n",
        "$BN(x_{i, k}) = \\gamma_{k} \\frac{x_{i, k} - \\mu_{B, k}}{\\sqrt{\\sigma^{2}_{B,k} + \\epsilon}} + \\beta_{k}$\n",
        "\n",
        "The intuitive idea behind BN is as follows: a neural network is trained using mini-batches. The distribution of inputs varies from one batch to the other. Difference in distributions between mini-batches can cause the training to be unstable and heavily dependant on the initial weights of the network. Therefore, this kind of transformation (transforming the inputs to be mean 0 and unit variance) guarantees that input distribution of each layer remains unchanged across mini-batches.\n",
        "\n",
        "More interestingly, we will learn how to code BN layer from scratch using PyTorch."
      ]
    },
    {
      "metadata": {
        "id": "x3MlbaKSaYVU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Library needed for visualization purposes\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "# Instantiate visualizer\n",
        "tb = TensorBoardColab(graph_path='./log')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NH2JP9URp1E3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### BatchNorm1d\n",
        "\n",
        "BN module for fully-connected hidden layers."
      ]
    },
    {
      "metadata": {
        "id": "0s9E5si1L2ip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Applies Batch Normalization over a 1D input (or 2D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C)\n",
        "  Output: (N, C)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm1d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm1d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4)\n",
        "  >>> out = bn(input)\n",
        "'''\n",
        "\n",
        "class BatchNorm1d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super(BatchNorm1d, self).__init__()\n",
        "    \n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    if self.affine:\n",
        "      # TODO declare scale param\n",
        "      # TODO declare shift param\n",
        "    \n",
        "    if self.track_running_stats:\n",
        "      # TODO declare running mean\n",
        "      # TODO declare running std\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \n",
        "    # transpose (N, C) to (C, N)\n",
        "    x = x.transpose(0, 1).contiguous().view(x.shape[1], -1)\n",
        "    \n",
        "    # calculate batch mean\n",
        "    # TODO\n",
        "    \n",
        "    # calculate batch std\n",
        "    # TODO\n",
        "    \n",
        "    # keep running statistics (moving average of mean and std)\n",
        "    if self.track_running_stats:\n",
        "      # TODO\n",
        "      # TODO\n",
        "    \n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      # TODO\n",
        "      # TODO\n",
        "    \n",
        "    # normalize the input activations\n",
        "    # TODO\n",
        "    \n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      # TODO\n",
        "    \n",
        "    return x.transpose(0, 1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WZjvl9dPqNn7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### BatchNorm2d\n",
        "\n",
        "BN module for convolutional layers"
      ]
    },
    {
      "metadata": {
        "id": "HPylQlV30-Rq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Applies Batch Normalization over a 2D or 3D input (4D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C, H, W)\n",
        "  Output: (N, C, H, W)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm2d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm2d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4, 5, 5)\n",
        "  >>> out = bn(input)\n",
        "'''\n",
        "\n",
        "class BatchNorm2d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super(BatchNorm2d, self).__init__()\n",
        "    \n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    if self.affine:\n",
        "      # TODO declare scale param\n",
        "      # TODO declare shift param\n",
        "    \n",
        "    if self.track_running_stats:\n",
        "      # TODO declare running mean\n",
        "      # TODO declare running std\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \n",
        "    # transpose (N, C, H, W) to (C, N, H, W)\n",
        "    x = x.transpose(0, 1)\n",
        "    \n",
        "    # store the shape\n",
        "    c, bs, h, w = x.shape\n",
        "    \n",
        "    # collapse all dimensions except the 'channel' dimension\n",
        "    # TODO\n",
        "    \n",
        "    # calculate batch mean\n",
        "    # TODO\n",
        "    \n",
        "    # calculate batch std\n",
        "    # TODO\n",
        "    \n",
        "    # keep running statistics (moving average of mean and std)\n",
        "    if self.track_running_stats:\n",
        "      # TODO\n",
        "      # TODO\n",
        "    \n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      # TODO\n",
        "      # TODO\n",
        "    \n",
        "    # normalize the input activations\n",
        "    # TODO\n",
        "    \n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      # TODO\n",
        "    \n",
        "    return x.view(c, bs, h, w).transpose(0, 1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "laMnRw6KqT2a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lenet-5 with Batch Norm\n",
        "\n",
        "We will be using BN layers for LeNet-5. BN layers are added right after the conv and fully-connected layers, except the output layer."
      ]
    },
    {
      "metadata": {
        "id": "lcUyscwXTvvt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LeNet(torch.nn.Module):\n",
        "  def __init__(self, norm=False):\n",
        "    super(LeNet, self).__init__()\n",
        "    self.norm = norm\n",
        "    \n",
        "    # input channel = 3, output channels = 6, kernel size = 5\n",
        "    # input image size = (32, 32), image output size = (28, 28)\n",
        "    self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(5, 5))\n",
        "    if self.norm:\n",
        "      # TODO\n",
        "    \n",
        "    # input channel = 6, output channels = 16, kernel size = 5\n",
        "    # input image size = (14, 14), output image size = (10, 10)\n",
        "    self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5))\n",
        "    if self.norm:\n",
        "      # TODO\n",
        "    \n",
        "    # input dim = 5 * 5 * 16 ( H x W x C), output dim = 120\n",
        "    self.fc3 = torch.nn.Linear(in_features=5 * 5 * 16, out_features=120)\n",
        "    if self.norm:\n",
        "      # TODO\n",
        "    \n",
        "    # input dim = 120, output dim = 84\n",
        "    self.fc4 = torch.nn.Linear(in_features=120, out_features=84)\n",
        "    if self.norm:\n",
        "      # TODO\n",
        "    \n",
        "    # input dim = 84, output dim = 10\n",
        "    self.fc5 = torch.nn.Linear(in_features=84, out_features=10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.conv1(x)\n",
        "    if self.norm:\n",
        "      # TODO\n",
        "    x = F.relu(x)\n",
        "    # Max Pooling with kernel size = 2\n",
        "    # output size = (14, 14)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "    \n",
        "    x = self.conv2(x)\n",
        "    if self.norm:\n",
        "      # TODO\n",
        "    x = F.relu(x)\n",
        "    # Max Pooling with kernel size = 2\n",
        "    # output size = (5, 5)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "    \n",
        "    # flatten the feature maps into a long vector\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    \n",
        "    x = self.fc3(x)\n",
        "    if self.norm:\n",
        "      # TODO\n",
        "    x = F.relu(x)\n",
        "    \n",
        "    x = self.fc4(x)\n",
        "    if self.norm:\n",
        "      # TODO\n",
        "    x = F.relu(x)\n",
        "    \n",
        "    x = self.fc5(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zYEVPMlIq187",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define cost function"
      ]
    },
    {
      "metadata": {
        "id": "YhxXGlp8VH2b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ujkEY2b3q45u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the optimizer"
      ]
    },
    {
      "metadata": {
        "id": "j4fzx5IIVIb0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uz-WKC9Cq7mt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and test functions"
      ]
    },
    {
      "metadata": {
        "id": "4EdzEauTVKFS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  net.train()\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # Reset the optimizer\n",
        "      \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "78f3GfZRq_IV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the function that fetches a data loader that is then used during iterative training."
      ]
    },
    {
      "metadata": {
        "id": "ayqqTsQZVSJt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=256, dataset='mnist'):\n",
        "  \n",
        "  # Prepare data transformations and then combine them sequentially\n",
        "  if dataset == 'mnist':\n",
        "    transform = list()\n",
        "    transform.append(T.ToTensor())                                              # converts Numpy to Pytorch Tensor\n",
        "    # TODO                                                                      # pad zeros to make MNIST 32 x 32\n",
        "    transform.append(T.Lambda(lambda x: x.repeat(3, 1, 1)))                     # to make MNIST RGB instead of grayscale\n",
        "    # TODO                                                                      # Normalizes the Tensors between [-1, 1]\n",
        "    transform = T.Compose(transform)                                            # Composes the above transformations into one.\n",
        "  elif dataset == 'svhn':\n",
        "    transform = list()\n",
        "    transform.append(T.ToTensor())                                              # converts Numpy to Pytorch Tensor\n",
        "    # TODO                                                                      # Normalizes the Tensors between [-1, 1]\n",
        "    transform = T.Compose(transform)                                            # Composes the above transformations into one.\n",
        "  \n",
        "  # Prepare dataset\n",
        "  if dataset == 'mnist':  \n",
        "    # Load data\n",
        "    full_training_data = torchvision.datasets.MNIST('./data/mnist', train=True, transform=transform, download=True) \n",
        "    test_data = torchvision.datasets.MNIST('./data/mnist', train=False, transform=transform, download=True)\n",
        "  elif dataset == 'svhn':\n",
        "    full_training_data = torchvision.datasets.SVHN('./data/svhn', split='train', transform=transform, download=True) \n",
        "    test_data = torchvision.datasets.SVHN('./data/svhn', split='test', transform=transform, download=True)\n",
        "  \n",
        "\n",
        "  # Create train and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples * 0.8 + 1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, drop_last=True)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "  \n",
        "  return train_loader, val_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SD4UDevjrEkX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Wrapping everything up\n",
        "\n",
        "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "metadata": {
        "id": "ZHdYGKLaWz5h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "  visualization_name: name of the tensorboard folder\n",
        "  dataset: which dataset to train\n",
        "  norm: whether to use batch normalization\n",
        "'''\n",
        "\n",
        "def main(batch_size=128, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.01, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50, \n",
        "         visualization_name='mnist',\n",
        "         dataset='mnist', \n",
        "         norm=False):\n",
        "  \n",
        "  train_loader, val_loader, test_loader = get_data(batch_size=batch_size, \n",
        "                                                   test_batch_size=batch_size, \n",
        "                                                   dataset=dataset)\n",
        "  \n",
        "  net = LeNet(norm=norm).to(device)\n",
        "  \n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  \n",
        "  # Add values to plots\n",
        "  tb.save_value('Loss/train_loss', visualization_name, 0, train_loss)\n",
        "  tb.save_value('Loss/val_loss', visualization_name, 0, val_loss)\n",
        "  tb.save_value('Accuracy/train_accuracy', visualization_name, 0, train_accuracy)\n",
        "  tb.save_value('Accuracy/val_accuracy', visualization_name, 0, val_accuracy)\n",
        "  \n",
        "  # Update plots \n",
        "  tb.flush_line(visualization_name)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    # Add values to plots\n",
        "    tb.save_value('Loss/train_loss', visualization_name, e + 1, train_loss)\n",
        "    tb.save_value('Loss/val_loss', visualization_name, e + 1, val_loss)\n",
        "    tb.save_value('Accuracy/train_accuracy', visualization_name, e + 1, train_accuracy)\n",
        "    tb.save_value('Accuracy/val_accuracy', visualization_name, e + 1, val_accuracy)\n",
        "    \n",
        "    # Update plots \n",
        "    tb.flush_line(visualization_name)\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xVdvvinZrJr4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train on MNIST without BN layers"
      ]
    },
    {
      "metadata": {
        "id": "HQ5hLnvOXXfJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "main(visualization_name='mnist', dataset='mnist')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eRcHGrWHrMy1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train on MNIST with BN layers"
      ]
    },
    {
      "metadata": {
        "id": "yPusNMELz3Qh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "main(visualization_name='mnist_bn', dataset='mnist', norm=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0igIuUcKrStL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train on SVHN without BN layers"
      ]
    },
    {
      "metadata": {
        "id": "_f92SRE0XZN8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "main(visualization_name='svhn', dataset='svhn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SuUa_ckgrPt5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train on SVHN with BN layers"
      ]
    },
    {
      "metadata": {
        "id": "3_7l_KhSz6Ox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "main(visualization_name='svhn_bn', dataset='svhn', norm=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}