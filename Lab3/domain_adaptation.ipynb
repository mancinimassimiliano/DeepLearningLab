{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "domain_adaptation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mancinimassimiliano/DeepLearningLab/blob/master/Lab3/domain_adaptation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jqhWaemXYXsB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction to Unsupervised Domain Adaption.\n",
        "\n",
        "### Domain Alignment Layers for Unsupervised Domain Adaptation.\n",
        "\n",
        "We have at disposal a source dataset (images with labels) and a target dataset (images *without* labels). Our goal is to train a classifier on the source and target dataset that can perform well on the target test dataset. This problem setting is known as **Unsupervised Domain Adaptation**.\n",
        "\n",
        "In this case the training set consists of the following:\n",
        "\n",
        "\n",
        "*   Source dataset: SVHN ~ 73k *labeled* data.\n",
        "*   Target dataset> MNIST ~ 60k *unlabeled* data, eaning no associated labels.\n",
        "\n",
        "Test set contains 10k MNIST test samples, which are used for evaluation.\n",
        "\n",
        "We will use Batch Norm based Domain Alignment (DA) Layers for performing domain adaptation.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CeDheQHyMCR7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bDvRWBAiCOgq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the network with Domain Alignment Layers.\n",
        "\n",
        "Each DA layer consist of domain specific Batch Normalization Layers followed by domain agnostic scale-shift operation. Note that the domain specific BN layer will accumulate the domain specific first and second order statistics, i.e., mean and std. It can achieved by setting `track_running_stats=True`.\n",
        "\n",
        "Details for BN can be found [here](https://pytorch.org/docs/stable/nn.html#batchnorm2d).\n",
        "\n",
        "### Network architecture adopted from [here](http://proceedings.mlr.press/v37/ganin15-supp.pdf)\n",
        "{Conv(3, 64, 5, 2) -> DA -> ReLU } ->MaxPool(3,2) -> {Conv(64, 64, 5, 2) -> DA -> ReLU } -> MaxPool(3,2) -> {Conv(64, 128, 5, 2) -> DA -> ReLU } -> {Linear(6272, 3072) -> DA -> ReLU -> Dropout} ->  {Linear(3072, 2048) -> DA -> ReLU -> Dropout} -> {Linear(2048, 10) -> DA}"
      ]
    },
    {
      "metadata": {
        "id": "45Fmwkl8CLG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DIALNet(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(DIALNet, self).__init__()\n",
        "\t\t# TODO layers\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\tif self.training:\n",
        "\t\t\t# TODO conv\n",
        "\t\t\t# TODO split source and target batches\n",
        "\t\t\t# TODO DA layer\n",
        "      # TODO Relu\n",
        "      # TODO Pooling\n",
        "\n",
        "\t\t\t# TODO conv\n",
        "\t\t\t# TODO split source and target batches\n",
        "\t\t\t# TODO DA layer\n",
        "      # TODO Relu\n",
        "      # TODO Pooling\n",
        "\n",
        "\t\t\t# TODO conv\n",
        "\t\t\t# TODO split source and target batches\n",
        "\t\t\t# TODO DA layer\n",
        "      # TODO Relu\n",
        "\n",
        "\t\t\tx = x.view(x.shape[0], -1)\n",
        "\t\t\t# TODO Linear\n",
        "\t\t\t# TODO split source and target batches\n",
        "\t\t\t# TODO DA layer\n",
        "      # TODO Relu\n",
        "      # TODO Dropout\n",
        "\n",
        "\t\t\t# TODO Linear\n",
        "\t\t\t# TODO split source and target batches\n",
        "\t\t\t# TODO DA layer\n",
        "      # TODO Relu\n",
        "      # TODO Dropout\n",
        "      \n",
        "\t\t\t# TODO Linear\n",
        "\t\t\t# TODO split source and target batches\n",
        "\t\t\t# TODO DA layer\n",
        "      \n",
        "\t\telse: # only for the target domain\n",
        "\t\t\t# TODO conv\n",
        "\t\t\t# TODO DA layer\n",
        "      # TODO Relu\n",
        "      # TODO Pooling\n",
        "\n",
        "\t\t\t# TODO conv\n",
        "\t\t\t# TODO DA layer\n",
        "      # TODO Relu\n",
        "      # TODO Pooling\n",
        "\n",
        "\t\t\t# TODO conv\n",
        "\t\t\t# TODO DA layer\n",
        "      # TODO Relu\n",
        "\n",
        "\t\t\tx = x.view(x.shape[0], -1)\n",
        "\t\t\t# TODO Linear\n",
        "\t\t\t# TODO DA layer\n",
        "      # TODO Relu\n",
        "      # TODO Dropout\n",
        "\n",
        "\t\t\t# TODO Linear\n",
        "\t\t\t# TODO DA layer\n",
        "      # TODO Relu\n",
        "      # TODO Dropout\n",
        "      \n",
        "\t\t\t# TODO Linear\n",
        "\t\t\t# TODO DA layer\n",
        "\t\treturn x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vggSe0nOHE3l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define cross-entropy loss function\n",
        "\n",
        "For the labeled source data we can compute the cross entropy loss\n",
        "\n",
        "$L^{s}(\\theta) = - \\frac{1}{m} \\displaystyle \\sum^{m}_{i=1} y_{i}\\log{p_i}$\n",
        "\n",
        "where $p_i$ is the output from the network for the $i^{th}$ input and $y_i$ is the corresponding ground truth."
      ]
    },
    {
      "metadata": {
        "id": "0kAlRo5gG5cl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_ce_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7xt4NUHGJoeA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Entropy loss function\n",
        "\n",
        "For the unlabeled target samples we can no longer compute the cross entropy loss because the labels are not available. Instead we will compute the Entropy loss, which is defined as follows:\n",
        "\n",
        "$L^{t}(\\theta) = - \\frac{1}{m} \\displaystyle \\sum^{m}_{i=1} p_{i}\\log{p_i}$\n",
        "\n",
        "where $p_i$ is the output from the network for the $i^{th}$ input.\n",
        "\n",
        "For more details read [this](https://arxiv.org/pdf/1704.08082.pdf)."
      ]
    },
    {
      "metadata": {
        "id": "4ExbsbDBJvB2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_entropy_loss(x):\n",
        "  p = F.softmax(x, dim=1)\n",
        "  q = F.log_softmax(x, dim=1)\n",
        "  b = p * q\n",
        "  b = -1.0 * b.sum(-1).mean()\n",
        "  return b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zgwU6qhHH75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the optimizer"
      ]
    },
    {
      "metadata": {
        "id": "whEOWuuQHKjo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6xrNlVOfHP3f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test function"
      ]
    },
    {
      "metadata": {
        "id": "CblMCosjHMud",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nTqK-0TNHZ1f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train function"
      ]
    },
    {
      "metadata": {
        "id": "zL3suQDeHcAU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(net, source_data_loader, target_data_loader, optimizer, \n",
        "          get_ce_cost_function, entropy_loss_weight, device='cuda:0'):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_ce_loss = 0.\n",
        "  cumulative_en_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  \n",
        "  # TODO target iterator\n",
        "\n",
        "  # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  net.train()\n",
        "  for batch_idx, (inputs_source, targets) in enumerate(source_data_loader):\n",
        "    \n",
        "    try:\n",
        "      # TODO get the target batch\n",
        "    except:\n",
        "      # TODO target iterator\n",
        "      # TODO get the target batch\n",
        "    \n",
        "    # TODO concat the source and target batches\n",
        "    \n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "    \n",
        "    # split the source and target outputs\n",
        "    # TODO\n",
        "    \n",
        "    # Apply the losses\n",
        "    # TODO cross entropy loss\n",
        "    # TODO entropy loss\n",
        "    \n",
        "    # TODO weighted sum of the losses\n",
        "    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    source_samples+=inputs_source.shape[0]\n",
        "    target_samples+=inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_ce_loss += ce_loss.item()\n",
        "    cumulative_en_loss += en_loss.item()\n",
        "    _, predicted = source_output.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_ce_loss/source_samples, cumulative_en_loss/target_samples, cumulative_accuracy/source_samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l6ZjgtVXOAaC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the function that fetches a data loader that is then used during iterative training."
      ]
    },
    {
      "metadata": {
        "id": "AAc-CQmDOFdN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "  \n",
        "  # Prepare data transformations and then combine them sequentially\n",
        "  transform_mnist = list()\n",
        "  transform_mnist.append(T.ToTensor())                                              # converts Numpy to Pytorch Tensor\n",
        "  transform_mnist.append(T.Lambda(lambda x: F.pad(x, (2, 2, 2, 2), 'constant', 0))) # pad zeros to make MNIST 32 x 32\n",
        "  transform_mnist.append(T.Lambda(lambda x: x.repeat(3, 1, 1)))                     # to make MNIST RGB instead of grayscale\n",
        "  transform_mnist.append(T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))    # Normalizes the Tensors between [-1, 1]\n",
        "  transform_mnist = T.Compose(transform_mnist)                                      # Composes the above transformations into one.\n",
        "  \n",
        "  transform_svhn = list()\n",
        "  transform_svhn.append(T.ToTensor())                                              # converts Numpy to Pytorch Tensor\n",
        "  transform_svhn.append(T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))    # Normalizes the Tensors between [-1, 1]\n",
        "  transform_svhn = T.Compose(transform_svhn)                                       # Composes the above transformations into one.\n",
        "  \n",
        "  # Prepare datasets\n",
        "  \n",
        "  # Load SVHN\n",
        "  source_training_data = torchvision.datasets.SVHN('./data/svhn', split='train', transform=transform_svhn, download=True) \n",
        "  \n",
        "  # Load MNIST\n",
        "  target_training_data = torchvision.datasets.MNIST('./data/mnist', train=True, transform=transform_mnist, download=True) \n",
        "  target_test_data = torchvision.datasets.MNIST('./data/mnist', train=False, transform=transform_mnist, download=True)\n",
        "  \n",
        "  # Initialize dataloaders\n",
        "  source_train_loader = torch.utils.data.DataLoader(source_training_data, batch_size, shuffle=True, drop_last=True)\n",
        "  target_train_loader = torch.utils.data.DataLoader(target_training_data, batch_size, shuffle=True, drop_last=True)\n",
        "  \n",
        "  target_test_loader = torch.utils.data.DataLoader(target_test_data, test_batch_size, shuffle=False)\n",
        "  \n",
        "  return source_train_loader, target_train_loader, target_test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWJbK4npPlkw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Wrapping everything up\n",
        "\n",
        "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "metadata": {
        "id": "9GcXu0EiOFUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "'''\n",
        "\n",
        "def main(batch_size=32, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.01, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50,\n",
        "         entropy_loss_weight=0.1):\n",
        "  \n",
        "  source_train_loader, target_train_loader, target_test_loader = get_data(batch_size)\n",
        "  \n",
        "  net = DIALNet().to(device)\n",
        "  \n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  cost_function = get_ce_cost_function()\n",
        "  \n",
        "  for e in range(epochs):\n",
        "    train_ce_loss, train_en_loss, train_accuracy = train(net=net, source_data_loader=source_train_loader, \n",
        "                                                         target_data_loader=target_train_loader, \n",
        "                                                         optimizer=optimizer, get_ce_cost_function=cost_function,\n",
        "                                                         entropy_loss_weight=entropy_loss_weight)\n",
        "    test_loss, test_accuracy = test(net, target_test_loader, cost_function)\n",
        "    \n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Train: CE loss {:.5f}, Entropy loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_en_loss, train_accuracy))\n",
        "    print('\\t Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2I6xQzBRSM55",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train!"
      ]
    },
    {
      "metadata": {
        "id": "I9qA7yLjSO5B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}