{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune_alexnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mancinimassimiliano/DeepLearningLab/blob/master/Lab3/finetune_alexnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UCJDwNS0dDlC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "### A quick guide to Transfer-learning\n",
        "\n",
        "Transfer learning allows you to \"*transfer*\" the knowledge gained while solving a task to solve another **related** task at hand. Fine-tuning a network, previously trained on a big dataset (e.g., ImageNet), to classify a given dataset containing limited annotated training samples is one of the simplest *transfer-learning* approaches.\n",
        "\n",
        "Specefically, in this lab we are going to see how to use a pre-trained Alexnet for the task of object recognition/classification. The will get you aquainted with the technique for fine-tuning any given pre-trained network."
      ]
    },
    {
      "metadata": {
        "id": "r6Qlk71Hctpl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mount your Google drive folder on Colab\n",
        "\n",
        "First things first, we are going to put data in google drive and then copy from google drive to colab local drive."
      ]
    },
    {
      "metadata": {
        "id": "dr1yEuKSPrpq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xoqaDOLSdEKD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download dataset\n",
        "\n",
        "After mounting the drive we will download OfficeHome dataset. \n",
        "\n",
        "Add [this](https://drive.google.com/file/d/10xMJi5uD8kVh9xkksq1pXngpUl-ckRBa/view?usp=sharing) .tar file to your Unitn google drive.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "m695n9_jStkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copy the .tar file from gdrive to local colab drive. This will make data loading faster.\n",
        "\n",
        "First create a directory with `!mkdir dataset` in your current path"
      ]
    },
    {
      "metadata": {
        "id": "RQ0TvpzVTTKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0i67CZuS33kF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp \"gdrive/My Drive/datasets/OfficeHomeDataset.tar\" dataset/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ujV7O9KzTfoH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unzip the .tar file\n",
        "\n",
        "```\n",
        "!tar -xf OfficeHomeDataset.tar\n",
        "```\n",
        "    Wait till its unzipped."
      ]
    },
    {
      "metadata": {
        "id": "KysfwRi-ema5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unzip here\n",
        "!tar -xf OfficeHomeDataset.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xxJU3U2qeqG3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Library needed for visualization purposes\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "# Instantiate visualizer\n",
        "tb = TensorBoardColab(graph_path='./log')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eAw9WPMdgZzU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Alexnet \n",
        "\n",
        "![Alexnet architecture](https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0106.png)\n",
        "\n",
        "This is the network architecture of [Alexnet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf). In this tutorial we are going to finetune a pre-trained Alexnet for classifying Office-Home images. Alexnet has been pre-trained on ILSVRC-2012 dataset, a dataset that has more than **1 million** images with 1000 classes.\n",
        "\n",
        "***A little bit about the Office-Home dataset.***\n",
        "\n",
        "Office Home has images from 4 different domains with each domain having 65 distinct categories. In this lab session we are going to use the `Art` domain.\n",
        "\n",
        "![Office-Home](http://hemanthdv.github.io/profile/images/DataCollage.jpg)\n",
        "\n",
        "***Steps for Fine tuning Alexnet***\n",
        "\n",
        "1.   Discard the final layer (or the output layer) of Alexnet which contains 1000 output neurons. \n",
        "2.   Randomly initialize the final layer with the number of output categories present in the dataset using `torch.nn.Linear`. In this case its 65 because OfficeHome has 65 classes. Keep all the other layers intact.\n",
        "3. Train the network with a low learning rate for the pre-initialized layers and and a higher learning rate for the newly initialized layer. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EyDL2C5EOmrt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### How to load datasets from folders containing images\n",
        "\n",
        "We have already seen that PyTorch's `torchvision` package provides some very basic dataloaders like `torchvision.datasets.MNIST`, `torchvision.datasets.SVHN`, etc. But it might happen (and is frequently the case) that we need to load our own datasets from folders.\n",
        "\n",
        "PyTorch is kind enough to provide `torchvision.datasets.ImageFolder` for loading datasets from folders with just one line of code. But we need to ensure that the images in the folders are arranged in a certain way. A sample format is shown below:\n",
        "\n",
        "OfficeHomeDataset\n",
        "\n",
        "        |\n",
        "        |--- Alarm_Clock\n",
        "        |                \n",
        "        |      |--- 00046.jpg\n",
        "        |      |--- 00050.jpg\n",
        "        |          \n",
        "        |--- Couch\n",
        "               | --- 00007.jpg\n",
        "               | --- 00023.jpg\n",
        " \n",
        " In other words, the parent folder (*OfficeHomeDataset*) contains the sub-folders (e.g *Alarm_Clock*, *Couch*). These sub-folders are the *classes*. Further, each sub-folder contains a bunch of images (eg. 00046.jpg, 00050.jpg for the class *Alarm_Clock*). Internally, PyTorch will assign a class label to each sub-folder and will also load the corresponding images.\n",
        " \n",
        " More details goes [here](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder).\n",
        " \n",
        "N.B. Your own folder structure might look different, so provide path to your parent folder accordingly."
      ]
    },
    {
      "metadata": {
        "id": "VEy32rMeVgx7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the function that fetches a data loader that is then used during iterative training.\n",
        "\n",
        "We are going to see some more PyTorch data transformations during loading the data."
      ]
    },
    {
      "metadata": {
        "id": "h8gyumMki8A6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments:\n",
        "  batch_size: mini batch size used during training\n",
        "  img_root: path to the dataset parent folder. \n",
        "            The folder just above the sub-folders or class folders\n",
        "'''\n",
        "\n",
        "def get_data(batch_size, img_root):\n",
        "  \n",
        "  # Prepare data transformations for the train loader\n",
        "  transform = list()\n",
        "  # TODO for image resize                                     # Resize each PIL image to 256 x 256\n",
        "  # TODO for image crop                                       # Randonly crop a 224 x 224 patch\n",
        "  # TODO for conversion to Tensor                             # converts Numpy to Pytorch Tensor\n",
        "  # TODO for normalization                                    # mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "                                                              # Normalize with ImageNet mean\n",
        "  # TODO for Compose                                          # Composes the above transformations into one.\n",
        "    \n",
        "  # Load data\n",
        "  # TODO\n",
        "  \n",
        "  # Create train and test splits\n",
        "  # We will create a 80:20 % train:test split\n",
        "  num_samples = len(officehome_dataset)\n",
        "  training_samples = int(num_samples * 0.8 + 1)\n",
        "  test_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, test_data = torch.utils.data.random_split(officehome_dataset, \n",
        "                                                           [training_samples, test_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False)\n",
        "  \n",
        "  return train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LQfpbouSan0O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Alexnet model\n",
        "\n",
        "PyTorch provides a bunch of pre-trained models whose parameters have been learned on ImageNet dataset. All these models can be found [here](https://pytorch.org/docs/stable/torchvision/models.html).\n",
        "\n",
        "We will load a pre-trained Alexnet using PyTorch's `torchvision` package. Then we will re-initialize the output layer suited for our classification task.\n",
        "\n",
        "Before that lets have a look at the [code](https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py) of Alexnet."
      ]
    },
    {
      "metadata": {
        "id": "XqUYywc6eH2d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments\n",
        "  num_classes: number of classes in the dataset.\n",
        "               This is equal to the number of output neurons.\n",
        "'''\n",
        "\n",
        "def initialize_alexnet(num_classes):\n",
        "  # load the pre-trained Alexnet\n",
        "  # TODO\n",
        "  \n",
        "  # get the number of neurons in the penultimate layer\n",
        "  # TODO for getting number of features\n",
        "  \n",
        "  # re-initalize the output layer\n",
        "  # TODO\n",
        "  \n",
        "  return alexnet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7RSi4GnMnv3U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize our custom Alexnet model"
      ]
    },
    {
      "metadata": {
        "id": "QI4VtFdVnzBg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(initialize_alexnet(65))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dvNjbRebk9X-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define cost function"
      ]
    },
    {
      "metadata": {
        "id": "i5SGHT3YlAHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPoVyKnmlEXu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the optimizer\n",
        "\n",
        "The optimizer will be different from the previous experiments.\n",
        "\n",
        "Previously the *learning rate* of all the layers of the network was the same.\n",
        "\n",
        "Now, we will have different layers learning at a different rate. The pre-trained layers need to be updated at a lesser rate than the newly initialized layer. Details are available [here](https://pytorch.org/docs/stable/optim.html)."
      ]
    },
    {
      "metadata": {
        "id": "LoqZLU0DlJ_N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  \n",
        "  # we will create two groups of weights, one for the newly initialized layer\n",
        "  # and the other for rest of the layers of the network\n",
        "  \n",
        "  # TODO for final layer weights\n",
        "  # TODO for rest of the net weights\n",
        "  \n",
        "  # we will iterate through the layers of the network\n",
        "  # TODO\n",
        "  \n",
        "  # so now we have divided the network weights into two groups.\n",
        "  # We will train the final_layer_weights with learning_rate = lr\n",
        "  # and rest_of_the_net_weights with learning_rate = lr / 10\n",
        "  \n",
        "  # TODO\n",
        "  \n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qW2kNnSkkxO3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and test functions"
      ]
    },
    {
      "metadata": {
        "id": "skcjnASok3w7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  \n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # Reset the optimizer\n",
        "      \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_quVeuqqM8U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Wrapping everything up\n",
        "\n",
        "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "metadata": {
        "id": "FK8A9alWqYZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "  num_classes: Number of classes in your dataset\n",
        "  visualization_name: Name of the visualization folder\n",
        "  img_root: The root folder of images\n",
        "'''\n",
        "\n",
        "def main(batch_size=128, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50, \n",
        "         num_classes=65, \n",
        "         visualization_name='alexnet_sgd', \n",
        "         img_root=None):\n",
        "  \n",
        "  train_loader, test_loader = get_data(batch_size=batch_size, \n",
        "                                       img_root=img_root)\n",
        "  \n",
        "  # TODO initialize network\n",
        "  \n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  \n",
        "  # Add values to plots\n",
        "  tb.save_value('Loss/train_loss', visualization_name, 0, train_loss)\n",
        "  tb.save_value('Loss/test_loss', visualization_name, 0, test_loss)\n",
        "  tb.save_value('Accuracy/train_accuracy', visualization_name, 0, train_accuracy)\n",
        "  tb.save_value('Accuracy/test_accuracy', visualization_name, 0, test_accuracy)\n",
        "    \n",
        "  # Update plots \n",
        "  tb.flush_line(visualization_name)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    # Add values to plots\n",
        "    tb.save_value('Loss/train_loss', visualization_name, e + 1, train_loss)\n",
        "    tb.save_value('Loss/test_loss', visualization_name, e + 1, test_loss)\n",
        "    tb.save_value('Accuracy/train_accuracy', visualization_name, se + 1, train_accuracy)\n",
        "    tb.save_value('Accuracy/test_accuracy', visualization_name, e + 1, test_accuracy)\n",
        "    \n",
        "    # Update plots \n",
        "    tb.flush_line(visualization_name)\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D-gT1YH4evqm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train!"
      ]
    },
    {
      "metadata": {
        "id": "-TrjNXtBq891",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "main(visualization_name='alexnet_sgd_0.01_RW', \n",
        "     img_root = '/content/dataset/OfficeHomeDataset_10072016/Real World')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}