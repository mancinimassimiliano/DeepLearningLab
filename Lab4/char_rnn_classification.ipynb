{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of char-rnn-classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mancinimassimiliano/DeepLearningLab/blob/master/Lab4/char_rnn_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM6jANconR6c",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial on Recurrent Neural Networks\n",
        "\n",
        "Recurrent Neural Networks (RNN) are models which are useful anytime we want to model sequences of data (e.g. video, text). In this tutorial (adapted from [here](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)), we will see how we can predict the language of a name using an RNN model taking single word characters as input. \n",
        "\n",
        "Specifically, we will train the network on a list of surnames from 18 languages of origin, and predict which language a name is from based on the spelling:\n",
        "\n",
        "```\n",
        "$ python predict.py Hinton\n",
        "(0.63) Scottish\n",
        "(0.22) English\n",
        "(0.02) Irish\n",
        "\n",
        "$ python predict.py Schmidhuber\n",
        "(0.83) German\n",
        "(0.08) Czech\n",
        "(0.07) Dutch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-viG81qXnR6m",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the Data\n",
        "\n",
        "The [link](https://download.pytorch.org/tutorial/data.zip) to download the needed data is provided within the official pytorch tutorial. The data must be downloaded and extracted in your virtual machine. We can do this through:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OUZbtZmznR6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KhH_f8lAnFB",
        "colab_type": "text"
      },
      "source": [
        "Under the downloaded directory there are 18 text files named as \"[Language].txt\". Each file contains a bunch of names, one name per line. In the following, we will take care of data preprocessing by :\n",
        "\n",
        "* Extracting all the names and numbers of categories from the files.\n",
        "* Converting from Unicode to ASCII each name.\n",
        "* Instantiating a dictionary containing all names (values) of a given language (key)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLv10cW_nR66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_filenames = glob.glob('data/names/*.txt')\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "print(unicode_to_ascii('Ślusàrski'))\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per language\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename).read().strip().split('\\n')\n",
        "    return [unicode_to_ascii(line) for line in lines]\n",
        "\n",
        "for filename in all_filenames:\n",
        "    category = filename.split('/')[-1].split('.')[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "print('n_categories =', n_categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6_4hcZfnR7g",
        "colab_type": "text"
      },
      "source": [
        "# Turning Names into Tensors\n",
        "\n",
        "A crucial point in this problem is how to define the input to the network. Since the network threats numbers and not plain text, we must convert text to numerical representation. To this extent we represent each letter as a one-hot vector of size `<1 x n_letters>`. A one-hot vector is filled with 0s except for a 1 at index of the current letter, e.g. `\"b\" = <0 1 0 0 0 ...>`.\n",
        "\n",
        "To make a word we join a bunch of those into a 2D matrix `<line_length x 1 x n_letters>`.\n",
        "\n",
        "That extra 1 dimension is because PyTorch assumes everything is in batches - we're just using a batch size of 1 here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-CGRQIMnR7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "  \n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letter_to_tensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    letter_index = all_letters.find(letter)\n",
        "    tensor[0][letter_index] = 1\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# Turn a line into a <line_length x n_letters>,\n",
        "# (or <line_length x 1 x n_letters> if the batch dimension is added)\n",
        "# of one-hot letter vectors\n",
        "def line_to_tensor(line,add_batch_dimension=True):\n",
        "    tensor = torch.zeros(len(line), n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        letter_index = all_letters.find(letter)\n",
        "        tensor[li][letter_index] = 1\n",
        "    if add_batch_dimension:\n",
        "      return tensor.unsqueeze(1)\n",
        "    else:\n",
        "      return tensor\n",
        "  \n",
        "  \n",
        "# Create a batch of samples given a list of lines\n",
        "def create_batch(lines):\n",
        "    tensors = []\n",
        "    for l in lines:\n",
        "      tensors.append(line_to_tensor(l,add_batch_dimension=False))\n",
        "      \n",
        "    padded_tensor = # TODO\n",
        "    return padded_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIP1vBlsnR8F",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Network\n",
        "\n",
        "Instantiate a simple recurrent neural network. The newtork should have a recurrent layer followed by a fully connected layer mapping the features of the recurrent unit to the output space (i.e. number of categories).\n",
        "\n",
        "\n",
        "\n",
        "To run a step of this network we need to pass an input (in our case, the Tensor for the current sequence/s) and a previous hidden state (which we initialize as zeros at first). We'll get back the logits (i.e. network activation before the softmax) for each each language.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY5kC4GlnR8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a simple recurrent network      \n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # Create the RNN layer. Be aware that the activation is included in most of them.\n",
        "        self.i2h = #TODO\n",
        "        \n",
        "        # Create the classifier \n",
        "        self.i2o = #TODO\n",
        "        \n",
        "    # Forward the whole sequence at once\n",
        "    def forward(self, input, hidden=None):\n",
        "        if hidden==None:\n",
        "          hidden = self.init_hidden(input.shape[1])\n",
        "          \n",
        "        output, _ = self.i2h # TODO: Be aware that the output changes with the chosen recurrent layer.\n",
        "        output = self.i2o(output[-1])\n",
        "        \n",
        "        \n",
        "        return output\n",
        "\n",
        "    # Instantiate the hidden state of the first element of the sequence dim: 1 x batch_size x hidden_size)\n",
        "    def init_hidden(self,shape=1):\n",
        "        return torch.zeros(1, shape, self.hidden_size)\n",
        "      \n",
        "      \n",
        "class SimpleRNNwithCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNNwithCell, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # Create the RNN unit/cell. Be aware that the activation is included in most of them.\n",
        "        self.i2h = #TODO\n",
        "        \n",
        "        # Create the classifier \n",
        "        self.i2o = #TODO\n",
        "    \n",
        "    def forward(self, input, hidden=None):\n",
        "        \n",
        "        if hidden==None:\n",
        "          hidden = self.init_hidden(input.shape[1])\n",
        "        \n",
        "        # Loop over the sequence \n",
        "        for i in range(input.shape[0]):\n",
        "          hidden = #TODO\n",
        "          output = #TODO\n",
        "          \n",
        "        return output\n",
        "\n",
        "    def init_hidden(self,shape=1):\n",
        "        return torch.zeros(shape, self.hidden_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-CAOp9snR8w",
        "colab_type": "text"
      },
      "source": [
        "# Preparing for Training\n",
        "\n",
        "Before going into training we should make a few helper functions. The first is to interpret the output of the network, which we know to be a logits of each category. We can use `Tensor.topk` to get the index of the greatest value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "b_OMIjrJnR8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def category_from_output(output):\n",
        "    top_n, top_i = output.data.topk(1)\n",
        "    category_i = top_i[0][0]\n",
        "    return all_categories[category_i], category_i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WajTvVVcnR86",
        "colab_type": "text"
      },
      "source": [
        "We will also want a quick way to get a training example (a name and its language):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tufexITVnR88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def random_training_pair(bs=1):\n",
        "    lines = []\n",
        "    categories = []\n",
        "    for b in range(bs):\n",
        "      category = random.choice(all_categories)\n",
        "      line = random.choice(category_lines[category])\n",
        "      \n",
        "      lines.append(line)\n",
        "      categories.append(category)\n",
        "      \n",
        "      \n",
        "    categories_tensor = torch.LongTensor([all_categories.index(c) for c in categories])\n",
        "    lines_tensor = create_batch(lines)\n",
        "    \n",
        "    return categories_tensor, lines_tensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLpMorCNnR9N",
        "colab_type": "text"
      },
      "source": [
        "# Training the Network\n",
        "\n",
        "Now all it takes to train this network is show it a bunch of examples, have it make guesses, and tell it if it's wrong.\n",
        "\n",
        "Since the output of the networks are logits and the task is classification, we can use a standard cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeMlEUaanR9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKVgVu2OnR9j",
        "colab_type": "text"
      },
      "source": [
        "Now we instantiate a standard training loop where we will:\n",
        "\n",
        "*   Forward the inpu to the network\n",
        "*   Compute the loss\n",
        "*   Backpropagate it\n",
        "*   Do a step of the optimizer\n",
        "*   Reset the optimizer/network's grad\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ARgzorlnR9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(rnn, optimizer, categories_tensor, lines_tensor):\n",
        "    # TODO\n",
        "    # - reset the optimizer \n",
        "    # - forward pass (output = ...)\n",
        "    # - compute loss  (loss = ...)\n",
        "    # - backward pass\n",
        "    # - update the parameters\n",
        "    \n",
        "    return output, loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyAxQJsmnR9v",
        "colab_type": "text"
      },
      "source": [
        "Now we just have to:\n",
        "*   Instantiate the network\n",
        "*   Instantiate the optimizer\n",
        "*   Run the training steps for a given number of iterations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "idgzdH84nR9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the network:\n",
        "n_hidden = 128\n",
        "rnn = # TODO\n",
        "\n",
        "# Initialize the optimizer\n",
        "learning_rate = 0.005 # Example: different LR could work better\n",
        "optimizer = # TODO\n",
        "\n",
        "# Initialize the training loop\n",
        "batch_size = 2\n",
        "n_iterations = 100000\n",
        "print_every = 5000\n",
        "\n",
        "# Keep track of losses\n",
        "current_loss = 0\n",
        "\n",
        "for iter in range(1, n_iterations + 1):\n",
        "    # Get a random training input and target\n",
        "    category_tensor, line_tensor = random_training_pair(bs=batch_size)\n",
        "    \n",
        "    # Process it through the train function\n",
        "    output, loss = train(rnn, optimizer, category_tensor, line_tensor)\n",
        "    \n",
        "    # Accumulate loss for printing\n",
        "    current_loss += loss\n",
        "    \n",
        "    # Print iteration number and loss\n",
        "    if iter % print_every == 0:\n",
        "        print('%d %d%% %.4f ' % (iter, iter / n_iterations * 100, current_loss/print_every))\n",
        "        current_loss = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk8TejEnnR-K",
        "colab_type": "text"
      },
      "source": [
        "# Running on User Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH25PHWmnR-R",
        "colab_type": "text"
      },
      "source": [
        "Finally, followith the original tutorial [in the Practical PyTorch repo](https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification) we instantiate a prediction function and test on some user defined inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpIQYrg-nR-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizer = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "def predict(input_line, n_predictions=3):\n",
        "    print('\\n> %s' % input_line)\n",
        "    output = rnn(line_to_tensor(input_line))\n",
        "    output = normalizer(output)\n",
        "    # Get top N categories\n",
        "    topv, topi = output.data.topk(n_predictions, 1, True)\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(n_predictions):\n",
        "        value = topv[0][i]\n",
        "        category_index = topi[0][i]\n",
        "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "        predictions.append([value, all_categories[category_index]])\n",
        "\n",
        "predict('Dovesky')\n",
        "predict('Jackson')\n",
        "predict('Satoshi')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}